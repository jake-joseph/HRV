{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4963f69c-b4dd-479a-adaa-d8eb20fc47b7",
   "metadata": {},
   "source": [
    "# Stimulation onsets/offsets extraction for the EKG-VNS study\n",
    "This script returns stimulation onsets and offsets from stim ch. It also reports the data summary.\n",
    "## Objective:\n",
    "- have a csv file containing 'correct' stim onsets/offset. There should be two stimulation per day for each patient\n",
    "- have a csv file reporting problematic dates per patient\n",
    "## Steps:\n",
    "- have box installed in the computer\n",
    "- import util functions\n",
    "- specify directory, target subjects, and output filenames (this is **important**!! otherwise overwriting happens)\n",
    "\n",
    "## Notes:\n",
    "- on mac, est 2hours per subj\n",
    "- we skip corrupted file and report them \n",
    "- ignore the 'compressed blabla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca36ed7-b143-4aa1-97e4-2b39fa9e0ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install pandas numpy matplotlib seaborn scipy biosppy hrv-analysis py-ecg-detectors  # check requirement in the first run!\n",
    "\n",
    "#@title Importing Modules\n",
    "# system imports\n",
    "import os\n",
    "#import sys\n",
    "import warnings\n",
    "# data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing \n",
    "from scipy import signal\n",
    "from scipy.ndimage import label\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import trapz\n",
    "from scipy import ndimage\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# bio signal processing \n",
    "from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values,get_time_domain_features\n",
    "from ecgdetectors import Detectors\n",
    "\n",
    "# misc\n",
    "import warnings\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "\n",
    "import gzip\n",
    "from scipy import signal\n",
    "import datetime\n",
    "from struct import pack, unpack_from, Struct\n",
    "\n",
    "\n",
    "\n",
    "unpack_b = Struct('<b').unpack_from\n",
    "unpack_w = Struct('<H').unpack_from\n",
    "unpack_s = Struct('<h').unpack_from\n",
    "unpack_f = Struct('<f').unpack_from\n",
    "unpack_d = Struct('<d').unpack_from\n",
    "unpack_dw = Struct('<L').unpack_from\n",
    "pack_b = Struct('<b').pack\n",
    "pack_w = Struct('<H').pack\n",
    "pack_s = Struct('<h').pack\n",
    "pack_f = Struct('<f').pack\n",
    "pack_d = Struct('<d').pack\n",
    "pack_dw = Struct('<L').pack\n",
    "\n",
    "\n",
    "def unpack_str(buf, pos):\n",
    "    strlen = unpack_dw(buf, pos)[0]\n",
    "    pos += 4\n",
    "    val = buf[pos:pos + strlen].decode('utf-8', 'ignore')\n",
    "    pos += strlen\n",
    "    return val, pos\n",
    "\n",
    "\n",
    "def pack_str(s):\n",
    "    sutf = s.encode('utf-8')\n",
    "    return pack_dw(len(sutf)) + sutf\n",
    "\n",
    "\n",
    "# 4 byte L (unsigned) l (signed)\n",
    "# 2 byte H (unsigned) h (signed)\n",
    "# 1 byte B (unsigned) b (signed)\n",
    "def parse_fmt(fmt):\n",
    "    if fmt == 1:\n",
    "        return 'f', 4\n",
    "    elif fmt == 2:\n",
    "        return 'd', 8\n",
    "    elif fmt == 3:\n",
    "        return 'b', 1\n",
    "    elif fmt == 4:\n",
    "        return 'B', 1\n",
    "    elif fmt == 5:\n",
    "        return 'h', 2\n",
    "    elif fmt == 6:\n",
    "        return 'H', 2\n",
    "    elif fmt == 7:\n",
    "        return 'l', 4\n",
    "    elif fmt == 8:\n",
    "        return 'L', 4\n",
    "    return '', 0\n",
    "\n",
    "\n",
    "class VitalFile:\n",
    "    def __init__(self, ipath, dtnames=None):\n",
    "        self.load_vital(ipath, dtnames)\n",
    "\n",
    "    def get_samples(self, dtname, interval=1):\n",
    "        if not interval:\n",
    "            return None\n",
    "\n",
    "        trk = self.find_track(dtname)\n",
    "        if not trk:\n",
    "            return None\n",
    "\n",
    "        # 리턴 할 길이\n",
    "        nret = int(np.ceil((self.dtend - self.dtstart) / interval))\n",
    "\n",
    "        if trk['type'] == 2:  # numeric track\n",
    "            ret = np.full(nret, np.nan)  # create a dense array\n",
    "            for rec in trk['recs']:  # copy values\n",
    "                idx = int((rec['dt'] - self.dtstart) / interval)\n",
    "                if idx < 0:\n",
    "                    idx = 0\n",
    "                elif idx > nret:\n",
    "                    idx = nret\n",
    "                ret[idx] = rec['val']\n",
    "            return ret\n",
    "        elif trk['type'] == 1:  # wave track\n",
    "            srate = trk['srate']\n",
    "            recs = trk['recs']\n",
    "\n",
    "            # 자신의 srate 만큼 공간을 미리 확보\n",
    "            nsamp = int(np.ceil((self.dtend - self.dtstart) * srate))\n",
    "            ret = np.full(nsamp, np.nan)\n",
    "\n",
    "            # 실제 샘플을 가져와 채움\n",
    "            for rec in recs:\n",
    "                sidx = int(np.ceil((rec['dt'] - self.dtstart) * srate))\n",
    "                eidx = sidx + len(rec['val'])\n",
    "                srecidx = 0\n",
    "                erecidx = len(rec['val'])\n",
    "                if sidx < 0:  # self.dtstart 이전이면\n",
    "                    srecidx -= sidx\n",
    "                    sidx = 0\n",
    "                if eidx > nsamp:  # self.dtend 이후이면\n",
    "                    erecidx -= (eidx - nsamp)\n",
    "                    eidx = nsamp\n",
    "                ret[sidx:eidx] = rec['val'][srecidx:erecidx]\n",
    "\n",
    "            # gain offset 변환\n",
    "            if trk['fmt'] > 2:  # 1: float, 2: double\n",
    "                ret *= trk['gain']\n",
    "                ret += trk['offset']\n",
    "\n",
    "            # 리샘플 변환\n",
    "            if srate != int(1 / interval + 0.5):\n",
    "                ret = np.take(ret, np.linspace(0, nsamp - 1, nret).astype(np.int64))\n",
    "\n",
    "            return ret\n",
    "\n",
    "        return None\n",
    "\n",
    "    def find_track(self, dtname):\n",
    "        dname = None\n",
    "        tname = dtname\n",
    "        if dtname.find('/') != -1:\n",
    "            dname, tname = dtname.split('/')\n",
    "\n",
    "        for trk in self.trks.values():  # find track\n",
    "            if trk['name'] == tname:\n",
    "                did = trk['did']\n",
    "                if did == 0 or not dname:\n",
    "                    return trk                    \n",
    "                if did in self.devs:\n",
    "                    dev = self.devs[did]\n",
    "                    if 'name' in dev and dname == dev['name']:\n",
    "                        return trk\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_vital(self, ipath, compresslevel=1):\n",
    "        f = gzip.GzipFile(ipath, 'wb', compresslevel=compresslevel)\n",
    "\n",
    "        # save header\n",
    "        if not f.write(b'VITA'):  # check sign\n",
    "            return False\n",
    "        if not f.write(pack_dw(3)):  # version\n",
    "            return False\n",
    "        if not f.write(pack_w(10)):  # header len\n",
    "            return False\n",
    "        if not f.write(self.header):  # save header\n",
    "            return False\n",
    "\n",
    "        # save devinfos\n",
    "        for did, dev in self.devs.items():\n",
    "            if did == 0: continue\n",
    "            ddata = pack_dw(did) + pack_str(dev['name']) + pack_str(dev['type']) + pack_str(dev['port'])\n",
    "            if not f.write(pack_b(9) + pack_dw(len(ddata)) + ddata):\n",
    "                return False\n",
    "\n",
    "        # save trkinfos\n",
    "        for tid, trk in self.trks.items():\n",
    "            ti = pack_w(tid) + pack_b(trk['type']) + pack_b(trk['fmt']) + pack_str(trk['name']) \\\n",
    "                + pack_str(trk['unit']) + pack_f(trk['mindisp']) + pack_f(trk['maxdisp']) \\\n",
    "                + pack_dw(trk['col']) + pack_f(trk['srate']) + pack_d(trk['gain']) + pack_d(trk['offset']) \\\n",
    "                + pack_b(trk['montype']) + pack_dw(trk['did'])\n",
    "            if not f.write(pack_b(0) + pack_dw(len(ti)) + ti):\n",
    "                return False\n",
    "\n",
    "            # save recs\n",
    "            for rec in trk['recs']:\n",
    "                rdata = pack_w(10) + pack_d(rec['dt']) + pack_w(tid)  # infolen + dt + tid (= 12 bytes)\n",
    "                if trk['type'] == 1:  # wav\n",
    "                    rdata += pack_dw(len(rec['val'])) + rec['val'].tobytes()\n",
    "                elif trk['type'] == 2:  # num\n",
    "                    fmtcode, fmtlen = parse_fmt(trk['fmt'])\n",
    "                    rdata += pack(fmtcode, rec['val'])\n",
    "                elif trk['type'] == 5:  # str\n",
    "                    rdata += pack_dw(0) + pack_str(rec['val'])\n",
    "\n",
    "                if not f.write(pack_b(1) + pack_dw(len(rdata)) + rdata):\n",
    "                    return False\n",
    "\n",
    "        # save trk order\n",
    "        if hasattr(self, 'trkorder'):\n",
    "            cdata = pack_b(5) + pack_w(len(self.trkorder)) + self.trkorder.tobytes()\n",
    "            if not f.write(pack_b(6) + pack_dw(len(cdata)) + cdata):\n",
    "                return False\n",
    "\n",
    "        f.close()\n",
    "        return True\n",
    "\n",
    "    def load_vital(self, ipath, dtnames=None):\n",
    "        if isinstance(dtnames, str):\n",
    "            if dtnames.find(','):\n",
    "                dtnames = dtnames.split(',')\n",
    "            else:\n",
    "                dtnames = [dtnames]\n",
    "\n",
    "        # dtnames: 로딩을 원하는 dtname 의 리스트. dtnames가 None 이면 트랙 목록만 읽혀짐\n",
    "        f = gzip.GzipFile(ipath, 'rb')\n",
    "\n",
    "        # parse header\n",
    "        if f.read(4) != b'VITA':  # check sign\n",
    "            return False\n",
    "\n",
    "        f.read(4)  # file version\n",
    "\n",
    "        buf = f.read(2)\n",
    "        if buf == b'':\n",
    "            return False\n",
    "\n",
    "        headerlen = unpack_w(buf, 0)[0]\n",
    "        self.header = f.read(headerlen)  # skip header\n",
    "        self.dgmt = unpack_s(self.header, 0)[0]  # ut - dgmt = localtime\n",
    "\n",
    "        # parse body\n",
    "        self.devs = {0: {}}  # device names. did = 0 represents the vital recorder\n",
    "        self.trks = {}\n",
    "        self.dtstart = 0\n",
    "        self.dtend = 0\n",
    "\n",
    "        try:\n",
    "            sel_tids = set()\n",
    "            while True:\n",
    "                buf = f.read(5)\n",
    "                if buf == b'':\n",
    "                    break\n",
    "                pos = 0\n",
    "\n",
    "                packet_type = unpack_b(buf, pos)[0]; pos += 1\n",
    "                packet_len = unpack_dw(buf, pos)[0]; pos += 4\n",
    "\n",
    "                buf = f.read(packet_len)\n",
    "                if buf == b'':\n",
    "                    break\n",
    "                pos = 0\n",
    "\n",
    "                if packet_type == 9:  # devinfo\n",
    "                    did = unpack_dw(buf, pos)[0]; pos += 4\n",
    "                    devtype, pos = unpack_str(buf, pos)\n",
    "                    name, pos = unpack_str(buf, pos)\n",
    "                    if len(buf) > pos + 4:  # port는 없을 수 있다\n",
    "                        port, pos = unpack_str(buf, pos)\n",
    "                    if not name:\n",
    "                        name = devtype\n",
    "                    self.devs[did] = {'name': name, 'type': devtype, 'port': port}\n",
    "                elif packet_type == 0:  # trkinfo\n",
    "                    did = col = 0\n",
    "                    montype = unit = ''\n",
    "                    gain = offset = srate = mindisp = maxdisp = 0.0\n",
    "                    tid = unpack_w(buf, pos)[0]; pos += 2\n",
    "                    trktype = unpack_b(buf, pos)[0]; pos += 1\n",
    "                    fmt = unpack_b(buf, pos)[0]; pos += 1\n",
    "                    tname, pos = unpack_str(buf, pos)\n",
    "\n",
    "                    if packet_len > pos:\n",
    "                        unit, pos = unpack_str(buf, pos)\n",
    "                    if packet_len > pos:\n",
    "                        mindisp = unpack_f(buf, pos)[0]\n",
    "                        pos += 4\n",
    "                    if packet_len > pos:\n",
    "                        maxdisp = unpack_f(buf, pos)[0]\n",
    "                        pos += 4\n",
    "                    if packet_len > pos:\n",
    "                        col = unpack_dw(buf, pos)[0]\n",
    "                        pos += 4\n",
    "                    if packet_len > pos:\n",
    "                        srate = unpack_f(buf, pos)[0]\n",
    "                        pos += 4\n",
    "                    if packet_len > pos:\n",
    "                        gain = unpack_d(buf, pos)[0]\n",
    "                        pos += 8\n",
    "                    if packet_len > pos:\n",
    "                        offset = unpack_d(buf, pos)[0]\n",
    "                        pos += 8\n",
    "                    if packet_len > pos:\n",
    "                        montype = unpack_b(buf, pos)[0]\n",
    "                        pos += 1\n",
    "                    if packet_len > pos:\n",
    "                        did = unpack_dw(buf, pos)[0]\n",
    "                        pos += 4\n",
    "\n",
    "                    dname = ''\n",
    "                    if did and did in self.devs:\n",
    "                        if did and did in self.devs:\n",
    "                            dname = self.devs[did]['name']\n",
    "                        dtname = dname + '/' + tname\n",
    "                    else:\n",
    "                        dtname = tname\n",
    "\n",
    "                    if dtnames:  # 사용자가 특정 트랙만 읽으라고 했을 때\n",
    "                        matched = False\n",
    "                        if dtname in dtnames:  # dtname (현재 읽고 있는 트랙명)이 dtnames에 지정된 것과 정확히 일치할 때\n",
    "                            matched = True\n",
    "                        else:\n",
    "                            for sel_dtname in dtnames:\n",
    "                                if dtname.endswith('/' + sel_dtname) or (dname + '/*' == sel_dtname): # 트랙명만 지정 or 특정 장비의 모든 트랙일 때\n",
    "                                    matched = True\n",
    "                                    sel_tids.add(tid)\n",
    "                                    break\n",
    "                                    \n",
    "                        if not matched:\n",
    "                            continue\n",
    "                        sel_tids.add(tid)  # 앞으로는 sel_tids 에서 체크한다\n",
    "\n",
    "                    # dtnames가 None 이거나 사용자가 원하는 sel 일 때\n",
    "                    self.trks[tid] = {'name': tname, 'dtname': dtname, 'type': trktype, 'fmt': fmt, 'unit': unit, 'srate': srate,\n",
    "                                      'mindisp': mindisp, 'maxdisp': maxdisp, 'col': col, 'montype': montype,\n",
    "                                      'gain': gain, 'offset': offset, 'did': did, 'recs': []}\n",
    "                elif packet_type == 1:  # rec\n",
    "                    infolen = unpack_w(buf, pos)[0]; pos += 2\n",
    "                    dt = unpack_d(buf, pos)[0]; pos += 8\n",
    "                    tid = unpack_w(buf, pos)[0]; pos += 2\n",
    "                    pos = 2 + infolen\n",
    "\n",
    "                    if self.dtstart == 0 or dt < self.dtstart:\n",
    "                        self.dtstart = dt\n",
    "                    \n",
    "                    # TODO: dtrec end 는 다를 수 있음 wav 읽어서 nsamp 로딩해야함\n",
    "                    if dt > self.dtend:\n",
    "                        self.dtend = dt\n",
    "\n",
    "                   # if not dtnames:  # dtnames 가 None 이면 트랙 목록만 읽혀짐\n",
    "                    #    continue\n",
    "\n",
    "                    if tid not in self.trks:  # 이전 정보가 없는 트랙이거나\n",
    "                        continue\n",
    "                 #   if tid not in sel_tids:  # 사용자가 트랙 지정을 한 경우\n",
    "                 #       continue\n",
    "\n",
    "                    trk = self.trks[tid]  \n",
    "\n",
    "                    fmtlen = 4\n",
    "                    # gain, offset 변환은 하지 않은 raw data 상태로만 로딩한다.\n",
    "                    # 항상 이 변환이 필요하지 않기 때문에 변환은 get_samples 에서 나중에 한다.\n",
    "                    if trk['type'] == 1:  # wav\n",
    "                        fmtcode, fmtlen = parse_fmt(trk['fmt'])\n",
    "                        nsamp = unpack_dw(buf, pos)[0]; pos += 4\n",
    "                        samps = np.ndarray((nsamp,), buffer=buf, offset=pos, dtype=np.dtype(fmtcode)); pos += nsamp * fmtlen\n",
    "                        trk['recs'].append({'dt': dt, 'val': samps})\n",
    "                    elif trk['type'] == 2:  # num\n",
    "                        fmtcode, fmtlen = parse_fmt(trk['fmt'])\n",
    "                        val = unpack_from(fmtcode, buf, pos)[0]; pos += fmtlen\n",
    "                        trk['recs'].append({'dt': dt, 'val': val})\n",
    "                    elif trk['type'] == 5:  # str\n",
    "                        pos += 4  # skip\n",
    "                        s, pos = unpack_str(buf, pos)\n",
    "                        trk['recs'].append({'dt': dt, 'val': s})\n",
    "                elif packet_type == 6:  # cmd\n",
    "                    cmd = unpack_b(buf, pos)[0]; pos += 1\n",
    "                    if cmd == 6:  # reset events\n",
    "                        evt_trk = self.find_track('/EVENT')\n",
    "                        if evt_trk:\n",
    "                            evt_trk['recs'] = []\n",
    "                    elif cmd == 5:  # trk order\n",
    "                        cnt = unpack_w(buf, pos)[0]; pos += 2\n",
    "                        self.trkorder = np.ndarray((cnt,), buffer=buf, offset=pos, dtype=np.dtype('H')); pos += cnt * 2\n",
    "\n",
    "        except EOFError as error:\n",
    "            print(error)\n",
    "            pass\n",
    "\n",
    "        # sorting tracks\n",
    "        # for trk in self.trks.values():\n",
    "        #     trk['recs'].sort(key=lambda r:r['dt'])\n",
    "\n",
    "        f.close()\n",
    "        return True\n",
    "\n",
    "\n",
    "def load_trk(tid, interval=1):\n",
    "    try:\n",
    "        url = 'https://api.vitaldb.net/' + tid\n",
    "        dtvals = pd.read_csv(url, na_values='-nan(ind)').values\n",
    "    except:\n",
    "        return np.empty(0)\n",
    "\n",
    "    if len(dtvals) == 0:\n",
    "        return np.empty(0)\n",
    "    \n",
    "    dtvals[:,0] /= interval  # convert time to row\n",
    "    nsamp = int(np.nanmax(dtvals[:,0])) + 1  # find maximum index (array length)\n",
    "    ret = np.full(nsamp, np.nan)  # create a dense array\n",
    "    \n",
    "    if np.isnan(dtvals[:,0]).any():  # wave track\n",
    "        if nsamp != len(dtvals):  # resample\n",
    "            ret = np.take(dtvals[:,1], np.linspace(0, len(dtvals) - 1, nsamp).astype(np.int64))\n",
    "        else:\n",
    "            ret = dtvals[:,1]\n",
    "    else:  # numeric track\n",
    "        for idx, val in dtvals:  # copy values\n",
    "            ret[int(idx)] = val\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def load_trks(tids, interval=1):\n",
    "    trks = []\n",
    "    maxlen = 0\n",
    "    for tid in tids:\n",
    "        trk = load_trk(tid, interval)\n",
    "        trks.append(trk)\n",
    "        if len(trk) > maxlen:\n",
    "            maxlen = len(trk)\n",
    "    if maxlen == 0:\n",
    "        return np.empty(0)\n",
    "    ret = np.full((maxlen, len(tids)), np.nan)  # create a dense array\n",
    "    for i in range(len(tids)):  # copy values\n",
    "        ret[:len(trks[i]), i] = trks[i]\n",
    "    return ret\n",
    "\n",
    "def vital_recs(ipath, dtnames, interval=0.3, return_timestamp=False, return_datetime=False):\n",
    "    if not dtnames:\n",
    "        return []\n",
    "\n",
    "    # \n",
    "    if isinstance(dtnames, str):\n",
    "        if dtnames.find(',') != -1:\n",
    "            dtnames = dtnames.split(',')\n",
    "        else:\n",
    "            dtnames = [dtnames]\n",
    "\n",
    "    vf = VitalFile(ipath, dtnames)\n",
    "\n",
    "    nrows = int(np.ceil((vf.dtend - vf.dtstart) / interval))\n",
    "    if not nrows:\n",
    "        return []\n",
    "\n",
    "    ret = []\n",
    "    for dtname in dtnames:\n",
    "        col = vf.get_samples(dtname, interval)\n",
    "        if col is None:\n",
    "            col = np.full(nrows, np.nan)\n",
    "        ret.append(col)\n",
    "    if not ret:\n",
    "        return []\n",
    "\n",
    "    # return time column\n",
    "    if return_datetime: # in this case, numpy array with object type will be returned\n",
    "        tzi = datetime.timezone(datetime.timedelta(minutes=-vf.dgmt))\n",
    "        dts = datetime.datetime.fromtimestamp(vf.dtstart, tzi)\n",
    "        dte = dts + datetime.timedelta(seconds=len(ret[0]))\n",
    "        ret.insert(0, [dts + datetime.timedelta(seconds=i*interval) for i in range(len(ret[0]))])\n",
    "    elif return_timestamp:\n",
    "        ret.insert(0, np.arange(vf.dtstart, vf.dtend, interval))\n",
    "\n",
    "    ret = np.transpose(ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def vital_trks(ipath):\n",
    "    # 트랙 목록만 읽어옴\n",
    "    ret = []\n",
    "    vf = VitalFile(ipath)\n",
    "    for trk in vf.trks.values():\n",
    "        tname = trk['name']\n",
    "        dname = ''\n",
    "        did = trk['did']\n",
    "        if did in vf.devs:\n",
    "            dev = vf.devs[did]\n",
    "            if 'name' in dev:\n",
    "                dname = dev['name']\n",
    "        ret.append(dname + '/' + tname)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# api files\n",
    "dftrks = None\n",
    "\n",
    "def load_case(tnames, caseid=None, interval=1):\n",
    "    global dftrks\n",
    "\n",
    "    if not caseid:\n",
    "        return None\n",
    "    if dftrks is None:\n",
    "        dftrks = pd.read_csv(\"https://api.vitaldb.net/trks\")\n",
    "\n",
    "    tids = []\n",
    "    for tname in tnames:\n",
    "        tid = dftrks[(dftrks['caseid'] == caseid) & (dftrks['tname'] == tname)]['tid'].values[0]\n",
    "        tids.append(tid)\n",
    "    \n",
    "    return load_trks(tids, interval)\n",
    "\n",
    "\n",
    "def load_cases(tnames, caseids=None, interval=1, maxcases=1):\n",
    "    global dftrks\n",
    "\n",
    "    # find the caseids which contains tnames\n",
    "    if not isinstance(tnames, list):\n",
    "        if isinstance(tnames, str):\n",
    "            tnames = tnames.split(',')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if interval == 0:\n",
    "        return None\n",
    "\n",
    "    if not caseids:\n",
    "        if dftrks is None:\n",
    "            dftrks = pd.read_csv(\"https://api.vitaldb.net/trks\")\n",
    "\n",
    "        # filter cases which don't have all tnames\n",
    "        caseids = None\n",
    "        for tname in tnames:\n",
    "            matched = set(dftrks[dftrks['tname'] == tname]['caseid'])\n",
    "            if caseids is None:\n",
    "                caseids = matched\n",
    "            else:\n",
    "                caseids = caseids & matched\n",
    "        \n",
    "    cases = {}\n",
    "    for caseid in caseids:\n",
    "        case = load_case(tnames, caseid, interval)\n",
    "        if case is None:\n",
    "            continue\n",
    "        if len(case) == 0:\n",
    "            continue\n",
    "        cases[caseid] = case\n",
    "        if len(cases) >= maxcases:\n",
    "            break\n",
    "\n",
    "    return cases\n",
    "\n",
    "#@title class that contains our vital data for future use\n",
    "\n",
    "class VitalDBData:\n",
    "    def __init__(self,vital_file):\n",
    "        vital_data = VitalFile(vital_file)\n",
    "        ecg_settings = vital_data.find_track('ECG_I')\n",
    "        self.sampling_rate = ecg_settings['srate']\n",
    "        self.ecg_I = vital_data.get_samples('ECG_I', 1/self.sampling_rate)\n",
    "        self.ecg_II = vital_data.get_samples('ECG_II', 1/self.sampling_rate)\n",
    "        self.ecg_III = vital_data.get_samples('ECG_III', 1/self.sampling_rate)\n",
    "        self.ch1 = vital_data.get_samples('CH1', 1/self.sampling_rate)\n",
    "#         self.ch1_normalized=zscore(self.ch1)\n",
    "        self.starttime = datetime.datetime.fromtimestamp(vital_data.dtstart)\n",
    "        self.endtime = datetime.datetime.fromtimestamp(vital_data.dtend)\n",
    "\n",
    "#         self.__loadCh1Info(vital_data,mu,sigm,20)\n",
    "        self.__removeNaNs()\n",
    "        \n",
    "    def __removeNaNs(self):\n",
    "        nan_time = np.isnan(self.ecg_I)  # this only remove nan time for ecg_I, there is possible ch1 contains nan\n",
    "        self.ecg_I[nan_time]=0\n",
    "        self.ecg_II[nan_time]=0\n",
    "        self.ecg_III[nan_time]=0\n",
    "        self.ch1[nan_time]=0\n",
    "        self.t = np.arange(len(self.ecg_I))/(self.sampling_rate)\n",
    "        \n",
    "def itpl_data_close_to0(data, lower_bound):\n",
    "    '''\n",
    "    data should be a positive numpy array\n",
    "    return data where both nan and value close to zero become zero\n",
    "    \n",
    "    e.g.\n",
    "    a = np.array([100, 200 ,0, 1, 0,8 ,52, 0,5])\n",
    "    itpl_data_close_to0(a,2)\n",
    "    \n",
    "    need help: g.tan@wustl.edu\n",
    "    '''\n",
    "    data[data < lower_bound] = 0\n",
    "    data[data == np.nan] = 0\n",
    "    zero_indices = data==0\n",
    "    interpolated = np.interp(zero_indices.nonzero()[0], (~zero_indices).nonzero()[0], data[~zero_indices])\n",
    "    data[zero_indices.nonzero()[0]] = interpolated\n",
    "    return data\n",
    "\n",
    "import numpy\n",
    "\n",
    "def smooth(x,window_len=1000,window='hanning'):\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError \n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    s=numpy.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=numpy.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('numpy.'+window+'(window_len)')\n",
    "\n",
    "    y=numpy.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'same') / w\n",
    "\n",
    "def unit_step(signal, condition):\n",
    "    return np.where(signal > condition, 1, 0)\n",
    "\n",
    "def err_analysis (report_error, subj, vit_dir, err_msg, timestamps_1day, stim_ch_1day_raw, stim_ch_1day, stim_detected, fig_fName):\n",
    "    report_error['subject'].append(subj)\n",
    "    report_error['date'].append(vit_dir)\n",
    "    report_error['err_msg'].append(err_msg)\n",
    "    fig = plt.figure(figsize=(18,8))\n",
    "    ax1 = plt.subplot(311)\n",
    "    ax2 = plt.subplot(312, sharex=ax1)\n",
    "    ax3 = plt.subplot(313, sharex=ax1)\n",
    "    y_stim4viz = np.zeros(timestamps_1day.shape[0])\n",
    "    y_stim4viz[1:(20 * 60 * 500)] = 1\n",
    "    ax1.plot(timestamps_1day, stim_ch_1day_raw, 'b')\n",
    "    ax1.legend(['raw_stim_ch'])\n",
    "    ax2.plot(timestamps_1day,y_stim4viz,'g', label='eg_20min')\n",
    "    ax2.plot(timestamps_1day, stim_ch_1day, 'b', label='thresholded_stim')\n",
    "    ax2.set_ylim([-0.2,1.2])   \n",
    "    ax2.legend(loc='upper right')\n",
    "    ax3.plot(timestamps_1day, stim_detected, 'r', label='identified_stim')\n",
    "    ax3.set_ylim([-0.2,1.2])\n",
    "    ax3.set(xlabel=\"time(s)\")\n",
    "    ax3.legend(loc='upper right')\n",
    "    fig.savefig(fig_fName)\n",
    "    plt.clf()\n",
    "    return report_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e16446-053c-45a4-8662-9e99883ecacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture capt  \n",
    "# comment the line above if the output does not annoy you\n",
    "# %%timeit # give us an estimation of the efficace, do not run as a cell magic command, it will loop the whole cell for multiple times\n",
    "# PARAMETERS\n",
    "lower_bound = 1e-3 # data in stim ch lower than this bound will be interpolated, modified inline\n",
    "interval_boundary = 30 * 500  # 30 seconds, used in stim identification algo\n",
    "padding_value = 100 * 500  # must > interval boundary\n",
    "tol_session_t = 22 * 500 * 60  #\n",
    "# savgol_wl = 100  # window length, ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html\n",
    "# savgol_mode = 'interp', savgol filter cost too much memories, not a good way\n",
    "search_dir = '/Users/ganshengt/Box/taVNS_clinical'  # change this if run in different computer\n",
    "err_report_fName = os.path.join(search_dir, 'report', 'error_report.csv')  # change this (right side of '=') if run in different computer\n",
    "stim_time_fName = os.path.join(search_dir, 'stim_time', 'stim_time_gt.csv')  # change this if run in different computer\n",
    "stim_time_questionable_fName = os.path.join(search_dir, 'stim_time', 'stim_time_questionable_gt.csv')  # change this if run in different computer\n",
    "\n",
    "vit_subject_dir= os.listdir(search_dir)\n",
    "vit_subject_dir = ['2020004','2020005', '2020006']  # please specify if you want to run the script on a subgroup of subjects\n",
    "# , '2020013', '2020014'\n",
    "stim_names = {'subject' : [],\n",
    "              'date':[],\n",
    "              'time' : [],\n",
    "              'event': []}\n",
    "stim_names_questionable = {'subject' : [],\n",
    "              'date':[],\n",
    "              'time' : [],\n",
    "              'event': []}\n",
    "report_error = {'subject' : [],\n",
    "              'date' : [],\n",
    "              'err_msg': []}\n",
    "\n",
    "for i,subj in enumerate(vit_subject_dir): #looping through taVNS folder, subj is patient\n",
    "    if not subj.isnumeric():\n",
    "        continue\n",
    "    mu = []\n",
    "    var=[]\n",
    "    thresh = []\n",
    "    vit_files_dir = os.listdir(os.path.join(search_dir,subj))\n",
    "    \n",
    "#     vit_files_dir = ['210801']  # 4 test, specify date that you want to run the script\n",
    "    for vit_dir in vit_files_dir: #looping through the days for patient\n",
    "        if not os.path.isdir(os.path.join(search_dir,subj,vit_dir)):\n",
    "            continue\n",
    "        vit_files=os.scandir(os.path.join(search_dir,subj,vit_dir))\n",
    "        stim_ch_1day = []\n",
    "        timestamps_1day = []\n",
    "#         date = parsed_file.starttime.date()  # use the file name\n",
    "        for j,vital_filename in enumerate(vit_files): # looping through file for that day\n",
    "            if not vital_filename.name.endswith('.vital'):  # jump over the tks file\n",
    "                continue\n",
    "            vital_file = os.path.join(search_dir,subj,vit_dir,vital_filename)\n",
    "            try: \n",
    "                vital_data = VitalFile(vital_file)\n",
    "                ch1 = vital_data.get_samples('CH1', 1/500.0)  # we are interested in the stim ch and timestamps\n",
    "                if ch1 is not None:\n",
    "                    parsed_file=VitalDBData(vital_file)\n",
    "                    stim_ch_1day += list(parsed_file.ch1)\n",
    "                    starttime_s = datetime.timedelta(hours=parsed_file.starttime.hour, minutes=parsed_file.starttime.minute, seconds=parsed_file.starttime.second, \n",
    "                                                     microseconds=parsed_file.starttime.microsecond).total_seconds()\n",
    "                    timestamps_1day += list(starttime_s + parsed_file.t)\n",
    "                    \n",
    "            except:\n",
    "                report_error['subject'].append(subj)\n",
    "                report_error['date'].append(vit_dir)\n",
    "                report_error['err_msg'].append('possible file corruption, loading error for' + vital_filename.name)\n",
    "                pass\n",
    "        \n",
    "        fig_fName = os.path.join(search_dir, 'report', subj + '_' + vit_dir + '_stim.pdf')  # pdf for higher resolution, change to jpeg\n",
    "        # switch to np for computational convenience\n",
    "        if np.sum(stim_ch_1day)==0:\n",
    "            report_error['subject'].append(subj)\n",
    "            report_error['date'].append(vit_dir)\n",
    "            report_error['err_msg'].append('no signal in stim ch')\n",
    "            continue\n",
    "            \n",
    "        stim_ch_1day_raw = np.array(stim_ch_1day).copy()\n",
    "        stim_ch_1day = np.array(np.absolute(stim_ch_1day))\n",
    "        timestamps_1day = np.array(timestamps_1day)\n",
    "#         stim_ch_1day = itpl_data_close_to0(stim_ch_1day, lower_bound)\n",
    "        stim_ch_1day = unit_step(stim_ch_1day, np.nanmax(stim_ch_1day)/2)\n",
    "        assert stim_ch_1day.shape == timestamps_1day.shape, 'mismatch in length, bugs?'\n",
    "        indices_relmax = signal.argrelmax(stim_ch_1day)[0]\n",
    "        # identify onsets and offsets by the interval length between relative max points\n",
    "        indices_relmax_diff = np.diff(indices_relmax)\n",
    "        if indices_relmax_diff.shape[0] < 1:  # in case no local maximum is found\n",
    "            report_error = err_analysis(report_error, subj, vit_dir, 'no relative maximum is found', timestamps_1day, \n",
    "                                        stim_ch_1day_raw, stim_ch_1day, stim_ch_1day_raw, fig_fName)\n",
    "            continue\n",
    "\n",
    "        indices_relmax_diff_l_padding = np.insert(indices_relmax_diff, 0, padding_value)  # padding in the left \n",
    "        indices_relmax_diff_r_padding = np.insert(indices_relmax_diff, indices_relmax_diff.shape[0], padding_value)  # padding in the right \n",
    "        indices_has_neighbor_l = indices_relmax_diff_l_padding < interval_boundary  # boolean array showing indices that have neighbor on their left \n",
    "        indices_has_neighbor_r = indices_relmax_diff_r_padding < interval_boundary\n",
    "        # indices_inside_stim = np.logical_and(indices_has_neighbor_l, indices_has_neighbor_r)\n",
    "        stim_onset = indices_relmax[np.logical_and(indices_has_neighbor_r, ~indices_has_neighbor_l)]  # stim onsets have neighbors on their right side but not left\n",
    "        stim_offset = indices_relmax[np.logical_and(indices_has_neighbor_l, ~indices_has_neighbor_r)]  # stim onsets have neighbors on their right side but not left\n",
    "        stim_detected = np.zeros(timestamps_1day.shape[0])\n",
    "        fig_fName = os.path.join(search_dir, 'report', subj + '_' + vit_dir + '_stim.pdf')  # pdf for higher resolution, change to jpeg\n",
    "        for i in range(stim_onset.shape[0]):\n",
    "            stim_detected[stim_onset[i]:stim_offset[i]]=1\n",
    "\n",
    "        if ((stim_offset - stim_onset) > tol_session_t).any(): # stim lasts more than 22 minutes, report error, we save onsets anyway for reference\n",
    "            report_error = err_analysis(report_error, subj, vit_dir, 'identified stim session more than 21 minutes', timestamps_1day, \n",
    "                                        stim_ch_1day_raw, stim_ch_1day, stim_detected, fig_fName)\n",
    "            for onset in stim_onset:\n",
    "                stim_names_questionable['subject'].append(subj)\n",
    "                stim_names_questionable['date'].append(vit_dir)\n",
    "                stim_names_questionable['time'].append(timestamps_1day[onset])\n",
    "                stim_names_questionable['event'].append('stim_onset')\n",
    "            for offset in stim_offset:\n",
    "                stim_names_questionable['subject'].append(subj)\n",
    "                stim_names_questionable['date'].append(vit_dir)\n",
    "                stim_names_questionable['time'].append(timestamps_1day[offset])\n",
    "                stim_names_questionable['event'].append('stim_offset')\n",
    "\n",
    "        elif stim_onset.shape[0] + stim_offset.shape[0] != 4:  # false detection \n",
    "            report_error = err_analysis(report_error, subj, vit_dir, 'detected more/less than 2 stimulation within one day', timestamps_1day, \n",
    "                                        stim_ch_1day_raw, stim_ch_1day, stim_detected, fig_fName)\n",
    "            for onset in stim_onset:\n",
    "                stim_names_questionable['subject'].append(subj)\n",
    "                stim_names_questionable['date'].append(vit_dir)\n",
    "                stim_names_questionable['time'].append(timestamps_1day[onset])\n",
    "                stim_names_questionable['event'].append('stim_onset')\n",
    "            for offset in stim_offset:\n",
    "                stim_names_questionable['subject'].append(subj)\n",
    "                stim_names_questionable['date'].append(vit_dir)\n",
    "                stim_names_questionable['time'].append(timestamps_1day[offset])\n",
    "                stim_names_questionable['event'].append('stim_offset')\n",
    "\n",
    "        else:\n",
    "            for onset in stim_onset:\n",
    "                stim_names['subject'].append(subj)\n",
    "                stim_names['date'].append(vit_dir)\n",
    "                stim_names['time'].append(timestamps_1day[onset])\n",
    "                stim_names['event'].append('stim_onset')\n",
    "            for offset in stim_offset:\n",
    "                stim_names['subject'].append(subj)\n",
    "                stim_names['date'].append(vit_dir)\n",
    "                stim_names['time'].append(timestamps_1day[offset])\n",
    "                stim_names['event'].append('stim_offset')\n",
    "\n",
    "df_report_error = pd.DataFrame(report_error)\n",
    "df_report_error.to_csv(err_report_fName)\n",
    "df_stim_time = pd.DataFrame(stim_names)\n",
    "df_stim_time.to_csv(stim_time_fName)\n",
    "df_stim_time_questionable = pd.DataFrame(stim_names_questionable)\n",
    "df_stim_time_questionable.to_csv(stim_time_questionable_fName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec5f4d-0c7f-45c6-a479-47500c433905",
   "metadata": {},
   "source": [
    "## For checking purpose: visualization for stim in one day for one subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1885336-df36-4da0-8a1a-e08cd42ac86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n",
      "Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f85a7ba8dc0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# manual inspection of the stim ch\n",
    "# PARAMETERS\n",
    "lower_bound = 1e-3 # data in stim ch lower than this bound will be interpolated, modified inline\n",
    "interval_boundary = 30 * 500  # 30 seconds, used in stim identification algo\n",
    "padding_value = 100 * 500  # must > interval boundary\n",
    "tol_session_t = 22 * 500 * 60  #\n",
    "# savgol_wl = 100  # window length, ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html\n",
    "# savgol_mode = 'interp', savgol filter cost too much memories, not a good way\n",
    "\n",
    "search_dir = '/Users/ganshengt/Box/taVNS_clinical'\n",
    "subj = '2020012'\n",
    "vit_dir = '210712'\n",
    "    \n",
    "\n",
    "vit_files=os.scandir(os.path.join(search_dir,subj,vit_dir))\n",
    "stim_ch_1day = []\n",
    "timestamps_1day = []\n",
    "#         date = parsed_file.starttime.date()  # use the file name\n",
    "for j,vital_filename in enumerate(vit_files): # looping through file for that day\n",
    "    if not vital_filename.name.endswith('.vital'):  # jump over the tks file\n",
    "        continue\n",
    "    vital_file = os.path.join(search_dir,subj,vit_dir,vital_filename)\n",
    "    try: \n",
    "        vital_data = VitalFile(vital_file)\n",
    "        ch1 = vital_data.get_samples('CH1', 1/500.0)  # we are interested in the stim ch and timestamps\n",
    "        if ch1 is not None:\n",
    "            parsed_file=VitalDBData(vital_file)\n",
    "            stim_ch_1day += list(parsed_file.ch1)\n",
    "            starttime_s = datetime.timedelta(hours=parsed_file.starttime.hour, minutes=parsed_file.starttime.minute, seconds=parsed_file.starttime.second, \n",
    "                                             microseconds=parsed_file.starttime.microsecond).total_seconds()\n",
    "            timestamps_1day += list(starttime_s + parsed_file.t)\n",
    "\n",
    "    except:\n",
    "        report_error['subject'].append(subj)\n",
    "        report_error['date'].append(vit_dir)\n",
    "        report_error['err_msg'].append('possible file corruption, loading error for' + vital_filename.name)\n",
    "        pass\n",
    "# switch to np for computational convenience\n",
    "stim_ch_1day = np.array(np.absolute(stim_ch_1day))\n",
    "timestamps_1day = np.array(timestamps_1day)\n",
    "#         stim_ch_1day = itpl_data_close_to0(stim_ch_1day, lower_bound)\n",
    "stim_ch_1day = unit_step(stim_ch_1day, np.max(stim_ch_1day)/2)\n",
    "\n",
    "indices_relmax = signal.argrelmax(stim_ch_1day)[0]\n",
    "# identify onsets and offsets by the interval length between relative max points\n",
    "indices_relmax_diff = np.diff(indices_relmax)\n",
    "indices_relmax_diff_l_padding = np.insert(indices_relmax_diff, 0, padding_value)  # padding in the left \n",
    "indices_relmax_diff_r_padding = np.insert(indices_relmax_diff, indices_relmax_diff.shape[0], padding_value)  # padding in the right \n",
    "indices_has_neighbor_l = indices_relmax_diff_l_padding < interval_boundary  # boolean array showing indices that have neighbor on their left \n",
    "indices_has_neighbor_r = indices_relmax_diff_r_padding < interval_boundary\n",
    "# indices_inside_stim = np.logical_and(indices_has_neighbor_l, indices_has_neighbor_r)\n",
    "stim_onset = indices_relmax[np.logical_and(indices_has_neighbor_r, ~indices_has_neighbor_l)]  # stim onsets have neighbors on their right side but not left\n",
    "stim_offset = indices_relmax[np.logical_and(indices_has_neighbor_l, ~indices_has_neighbor_r)]  # stim onsets have neighbors on their right side but not left\n",
    "stim_detected = np.zeros(timestamps_1day.shape[0])\n",
    "for i in range(stim_onset.shape[0]):\n",
    "    stim_detected[stim_onset[i]:stim_offset[i]]=1\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "plt.ylim([-0.2,1.2])\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212, sharex=ax1)\n",
    "ax1.plot(timestamps_1day, stim_ch_1day, 'b')\n",
    "y_stim4viz = np.zeros(timestamps_1day.shape[0])\n",
    "y_stim4viz[1:(20 * 60 * 500)] = 1\n",
    "ax1.plot(timestamps_1day,y_stim4viz,'g')\n",
    "ax1.legend(['raw_stim_ch', 'eg_20minute'])\n",
    "ax2.plot(timestamps_1day, stim_detected, 'r')\n",
    "ax2.legend(['identified_stim'], loc ='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23fade-578a-484e-94a2-0b3cf9638a57",
   "metadata": {},
   "source": [
    "## statistic of validated data\n",
    "$$rate_{identification} = \\frac{\\#(days\\, in\\, which\\, 1-2 stim\\, is\\, identified)}{\\#(recorded\\, days)} $$\n",
    "$$rate_{successful\\,identification} = \\frac{\\#(identified\\, stim)}{\\#(stim\\, that\\, is\\, supposed\\, to\\, deliever)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0e259ab-1f0d-445b-97a7-aa326fd16c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj: 2020004\n",
      "r_day: 0.7857142857142857\n",
      "r_stim: 0.7142857142857143\n",
      "subj: 2020005\n",
      "r_day: 0.8\n",
      "r_stim: 0.775\n",
      "subj: 2020006\n",
      "r_day: 0.5714285714285714\n",
      "r_stim: 0.5714285714285714\n",
      "subj: 2020007\n",
      "r_day: 0.6666666666666666\n",
      "r_stim: 0.5833333333333334\n",
      "subj: 2020008\n",
      "r_day: 0.75\n",
      "r_stim: 0.6875\n",
      "subj: 2020009\n",
      "r_day: 0.0\n",
      "r_stim: 0.0\n",
      "subj: 2020010\n",
      "r_day: 0.8\n",
      "r_stim: 0.8\n",
      "subj: 2020011\n",
      "r_day: 1.0\n",
      "r_stim: 0.8846153846153846\n",
      "subj: 2020012\n",
      "r_day: 0.5454545454545454\n",
      "r_stim: 0.4772727272727273\n",
      "subj: 2020013\n",
      "r_day: 1.0\n",
      "r_stim: 0.8571428571428571\n",
      "subj: 2020014\n",
      "r_day: 0.9\n",
      "r_stim: 0.85\n",
      "subj: 2020015\n",
      "r_day: 0.9565217391304348\n",
      "r_stim: 0.9347826086956522\n",
      "summary\n",
      "total number of days: 142\n",
      "total number of identified days: 112\n",
      "r_day: 0.7887323943661971\n",
      "r_stim: 0.7359154929577465\n"
     ]
    }
   ],
   "source": [
    "# import openpyxl\n",
    "# export report that summarizes the data processing\n",
    "report_data_processing = {'subject' : [],\n",
    "                          'days of session':[], 'identified days of session':[], 'rate of identified days':[], 'stimulation delivered':[], 'identified stimulation':[],\n",
    "                          'rate of identified stimulations': []}\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "stim_time_sum_fName = os.path.join('/Users/ganshengt/Box/taVNS_clinical/stim_time', 'stim_time_sum.txt')\n",
    "df_stim_time_sum = pd.read_csv(stim_time_sum_fName, encoding='utf-16', sep=\"\\t\", usecols=np.arange(4))\n",
    "search_dir = '/Users/ganshengt/Box/taVNS_clinical' \n",
    "vit_subject_dir= os.listdir(search_dir)\n",
    "total_number_day = 0\n",
    "total_number_day_identified = 0\n",
    "total_number_stim_identified = 0\n",
    "for i,subj in enumerate(vit_subject_dir):\n",
    "    total_number_day_subj = 0\n",
    "    if not os.path.isdir(os.path.join(search_dir,subj)):\n",
    "        continue\n",
    "    try: \n",
    "        int(subj)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    vit_files_dir = os.listdir(os.path.join(search_dir,subj))\n",
    "    for vit_dir in vit_files_dir: #looping through the days for patient\n",
    "        if not os.path.isdir(os.path.join(search_dir,subj,vit_dir)):\n",
    "            continue\n",
    "        total_number_day += 1\n",
    "        total_number_day_subj += 1\n",
    "    total_number_day_identified += np.unique(df_stim_time_sum[df_stim_time_sum['subject']==int(subj)]['date']).shape[0] \n",
    "    total_number_stim_identified += df_stim_time_sum[df_stim_time_sum['subject']==int(subj)].shape[0]\n",
    "    r_day = np.unique(df_stim_time_sum[df_stim_time_sum['subject']==int(subj)]['date']).shape[0] / total_number_day_subj\n",
    "    r_stim = df_stim_time_sum[df_stim_time_sum['subject']==int(subj)].shape[0] / 4 / total_number_day_subj\n",
    "    report_data_processing['subject'].append(subj)\n",
    "    report_data_processing['days of session'].append(total_number_day_subj)\n",
    "    report_data_processing['identified days of session'].append(np.unique(df_stim_time_sum[df_stim_time_sum['subject']==int(subj)]['date']).shape[0])\n",
    "    report_data_processing['rate of identified days'].append(r_day)\n",
    "    report_data_processing['stimulation delivered'].append(total_number_day_subj * 4)\n",
    "    report_data_processing['identified stimulation'].append(df_stim_time_sum[df_stim_time_sum['subject']==int(subj)].shape[0])\n",
    "    report_data_processing['rate of identified stimulations'].append(r_stim)\n",
    "    print('subj: {}'.format(subj))\n",
    "    print('r_day: {}'.format(r_day))\n",
    "    print('r_stim: {}'.format(r_stim))\n",
    "# summarize all the subjects\n",
    "report_data_processing['subject'].append('summary')\n",
    "report_data_processing['days of session'].append(total_number_day)\n",
    "report_data_processing['identified days of session'].append(total_number_day_identified)\n",
    "report_data_processing['rate of identified days'].append(total_number_day_identified / total_number_day)\n",
    "report_data_processing['stimulation delivered'].append(total_number_day * 4)\n",
    "report_data_processing['identified stimulation'].append(total_number_stim_identified)\n",
    "report_data_processing['rate of identified stimulations'].append(total_number_stim_identified / total_number_day / 4)\n",
    "print('summary')\n",
    "print('total number of days: {}'.format(total_number_day))\n",
    "print('total number of identified days: {}'.format(total_number_day_identified))\n",
    "print('r_day: {}'.format(total_number_day_identified / total_number_day))\n",
    "print('r_stim: {}'.format(total_number_stim_identified / total_number_day / 4))\n",
    "df_report_data_processing = pd.DataFrame(report_data_processing)\n",
    "df_report_data_processing.to_csv(os.path.join('/Users/ganshengt/Box/taVNS_clinical/report', 'report_data_processing.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
