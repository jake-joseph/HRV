{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0545222f",
   "metadata": {},
   "source": [
    "# Effects of taVNS on HRV \n",
    "## Discription\n",
    "2 groups (true label temporally unknown, each patient received VNS for several days during which there were 2 sessions of stimulation). Subject ID from 2020004 to 2020015.\n",
    "\n",
    "**Warning**: Both the number of days and the number of sessions depend on the subject.\n",
    "## Objective\n",
    "- To see the Effects of taVNS on HRV\n",
    "- Unsupervised clustering to see whether we can distinguise the two groups.\n",
    "\n",
    "## Input\n",
    "- nn_interval file (.pickle) for each subject\n",
    "- info csv (stimulation_timestamped.csv) indicating when the stimulation begins \n",
    "**Warning**: the info file was automatically generated. It needs a second check.\n",
    "\n",
    "## Output\n",
    "- For each group (patient), we explore the effect of VNS on HRV (boxplot, pre/during/post-stim)\n",
    "- Taking day as a variable, two independent variable: day and session\n",
    "- clustering including Features engineering (e.g. $HRV_{during} - \\frac{HRV_{pre} + HRV_{post}}{2})$ per patient) and unsupervised learning (e.g. kmeans, DBSCAN)\n",
    "\n",
    "## Workflow\n",
    "### 1. double check df_info\n",
    "- the length of the stimulation should be 20 min\n",
    "- there should be two stimulations per patient in a single day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80626deb",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e46b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "data_dir = os.path.expanduser(\"~/Desktop/GT/ECG_VNS/data\")\n",
    "# os.path.exists(data_dir)  # test\n",
    "\n",
    "info_filename = os.path.join(data_dir, 'stimulation_timestamped.csv')\n",
    "df_info = pd.read_csv(info_filename, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de673b",
   "metadata": {},
   "source": [
    "## filter bad rows in df_info (drop rows)\n",
    "### Untackled problem:\n",
    "- in this version, data were discarded if multipul plausible stimulation onsets were found\n",
    "### improvement 2 implement:\n",
    "- loops for list creation and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccee4c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw by raw scanning\n",
    "# criteria: 20 min duration\n",
    "# timestamp format: 2/22/21 20:18\n",
    "tolerence = 100 # 100 seconds\n",
    "VNS_duration = 1200 # 1200 seconds\n",
    "\n",
    "list_subj2filtered_df = []\n",
    "list_date2filtered_df = []\n",
    "list_time_s2filtered_df = []\n",
    "list_switch2filtered_df = []  # 1 stands for on, 0 stands for off\n",
    "\n",
    "array_subjs = df_info['subject'].unique()\n",
    "for subj in array_subjs:\n",
    "    df_per_subj = df_info[df_info['subject'] == subj]\n",
    "    list_date = []\n",
    "    list_time_s = []\n",
    "    for index, row in df_per_subj.iterrows():\n",
    "        # month/day/year\n",
    "#         date = row['timestamp'].split('/')[0] + '/' + row['timestamp'].split('/')[1] + row['timestamp'].split('/')[2]\n",
    "        date = row['timestamp'].split(' ')[0] \n",
    "        time = row['timestamp'].split('/')[2].split(' ')[1]\n",
    "        hour = int(time.split(':')[0])\n",
    "        minute = int(time.split(':')[1])\n",
    "        time_s = datetime.timedelta(hours=hour, minutes=minute).total_seconds()\n",
    "        list_date.append(date)\n",
    "        list_time_s.append(time_s)\n",
    "    list_time_s = np.array(list_time_s)\n",
    "    list_date = np.array(list_date)\n",
    "    _, unique_indices = np.unique(list_time_s, return_index=True)\n",
    "    unique_indices = np.sort(unique_indices)\n",
    "    # make sure that there are either 2 or 4 stim each day\n",
    "    list_time_s = list_time_s[unique_indices]\n",
    "    list_date = list_date[unique_indices]\n",
    "    \n",
    "    list_time_s_updated = []\n",
    "    list_date_updated = []\n",
    "    for date in np.unique(list_date):\n",
    "        indices2search = np.where(list_date == date)[0]\n",
    "        find_stim_onset = False\n",
    "        num_stim_found = 0\n",
    "        for i in indices2search:\n",
    "            for j in indices2search[np.where(indices2search == i)[0][0] + 1:]:\n",
    "                if (VNS_duration - tolerence) < (list_time_s[j] - list_time_s[i]) < (VNS_duration + tolerence):\n",
    "                    list_time_s_updated.append(list_time_s[i])\n",
    "                    list_time_s_updated.append(list_time_s[j])\n",
    "                    find_stim_onset = True\n",
    "                    num_stim_found +=1\n",
    "        \n",
    "                        \n",
    "        if num_stim_found > 2:  # more than two stim onsets were found\n",
    "#             list_date_updated = list_date_updated[list_date_updated != date]\n",
    "            list_time_s_updated = list_time_s_updated[:-2 * num_stim_found]\n",
    "        else:\n",
    "            for i in range(num_stim_found):\n",
    "                list_date_updated.append(date)\n",
    "    list_subj2filtered_df = list_subj2filtered_df + [subj] * len(list_time_s_updated)\n",
    "    list_date2filtered_df = list_date2filtered_df + [date for date in list_date_updated for _ in (0, 1)]\n",
    "    list_time_s2filtered_df = list_time_s2filtered_df + list_time_s_updated\n",
    "    list_switch2filtered_df = list_switch2filtered_df + [i for j in range(len(list_date_updated)) for i in [1, 0] ]\n",
    "\n",
    "# we create new df from arrays here\n",
    "df_info_filtered = pd.DataFrame(data=np.array([np.array(list_subj2filtered_df), np.array(list_date2filtered_df),\n",
    "                                               np.array(list_time_s2filtered_df), \n",
    "                                      np.array(list_switch2filtered_df)]).T,\n",
    "                                    columns=['subj', 'date', 'time_s', 'switch'])\n",
    "df_info_filtered.to_csv(os.path.join(data_dir, 'stimulation_timestamped_filtered.csv'))\n",
    "# Test properties here via assert. This applies when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaaf497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>date</th>\n",
       "      <th>time_s</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/22/21</td>\n",
       "      <td>73080.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/22/21</td>\n",
       "      <td>74340.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/23/21</td>\n",
       "      <td>75780.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/23/21</td>\n",
       "      <td>77040.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/24/21</td>\n",
       "      <td>27300.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/24/21</td>\n",
       "      <td>28560.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/24/21</td>\n",
       "      <td>71700.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/24/21</td>\n",
       "      <td>72960.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>31800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>33060.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>69780.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020004</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>71040.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subj     date   time_s switch\n",
       "0   2020004  2/22/21  73080.0      1\n",
       "1   2020004  2/22/21  74340.0      0\n",
       "2   2020004  2/23/21  75780.0      1\n",
       "3   2020004  2/23/21  77040.0      0\n",
       "4   2020004  2/24/21  27300.0      1\n",
       "5   2020004  2/24/21  28560.0      0\n",
       "6   2020004  2/24/21  71700.0      1\n",
       "7   2020004  2/24/21  72960.0      0\n",
       "8   2020004  2/25/21  31800.0      1\n",
       "9   2020004  2/25/21  33060.0      0\n",
       "10  2020004  2/25/21  69780.0      1\n",
       "11  2020004  2/25/21  71040.0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how it looks like\n",
    "# load the csv if necessary\n",
    "df_info_filtered.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbfced",
   "metadata": {
    "tags": []
   },
   "source": [
    "## extract pre/during/post-stm from pickle files\n",
    "Output format:\n",
    "\n",
    "| subj  | date | time |    session      | nn_interval|\n",
    "| ----- | ---- | ---- | --------------- | ---------- |\n",
    "|2020014| 2/23 |am/pm | pre/during/post |    value   |\n",
    "\n",
    "**Be careful, this might take 8h on mac**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a7206e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noon_s = 12*60*60\n",
    "def am_or_pm(time_s):\n",
    "    if 0 < time_s < noon_s:\n",
    "        output = 'am'\n",
    "    else:\n",
    "        output = 'pm'\n",
    "    return output\n",
    "\n",
    "def datetime2time_s(datetime_series):\n",
    "    '''\n",
    "    datetime is a pd series, no vectorized operation is possible\n",
    "    '''\n",
    "    time_s = []\n",
    "    for i in datetime_series:\n",
    "        hours = i.hour\n",
    "        minutes = i.minute\n",
    "        seconds = i.second\n",
    "        time_s.append(datetime.timedelta(hours=hours, minutes=minutes, \n",
    "                                seconds=seconds).total_seconds())\n",
    "    return time_s\n",
    "\n",
    "subj2df = []\n",
    "date2df = []\n",
    "time2df = []\n",
    "session2df = []\n",
    "nn_interval2df = []\n",
    "for subj_id in array_subjs:\n",
    "    nn_filename = os.path.join(data_dir, str(subj_id) + '.pickle')\n",
    "    df = pd.read_pickle(nn_filename)\n",
    "#     # truncate to speed up\n",
    "#     df = df.truncate(before=175000, after= 176000)\n",
    "#     # reindex by time_s\n",
    "#     df['time_s_idx'] = datetime2time_s(df['timestamp'])\n",
    "#     df.set_index('time_s_idx')\n",
    "    df_info_per_subj = df_info_filtered[df_info_filtered['subj'] == str(subj_id)]\n",
    "    dates = pd.unique(df_info_per_subj['date'])\n",
    "    df_info_per_subj_day = df_info_per_subj[df_info_per_subj['date'] == date]\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # date conversion\n",
    "        date = row['timestamp'].strftime('%-m/%d/%y')\n",
    "        time_s = datetime.timedelta(hours=row['timestamp'].hour, minutes=row['timestamp'].minute, \n",
    "                            seconds = row['timestamp'].second).total_seconds()\n",
    "        \n",
    "        df_info_per_subj_day = df_info_per_subj[df_info_per_subj['date'] == date]\n",
    "        onset_pre_VNS = df_info_per_subj_day[df_info_per_subj_day['switch'] == '1'].reset_index()['time_s'].astype(float)\\\n",
    "        - VNS_duration\n",
    "        offset_pre_VNS = df_info_per_subj_day[df_info_per_subj_day['switch'] == '1'].reset_index()['time_s'].astype(float)\n",
    "\n",
    "        onset_post_VNS = df_info_per_subj_day[df_info_per_subj_day['switch'] == '0'].reset_index()['time_s'].astype(float)\n",
    "        offset_post_VNS = df_info_per_subj_day[df_info_per_subj_day['switch'] == '0'].reset_index()['time_s'].astype(float)\\\n",
    "        + VNS_duration\n",
    "        time = am_or_pm(time_s)\n",
    "        \n",
    "        for i in range(int(df_info_per_subj_day.shape[0]/2)):  # we have one stim in the morning, one in the afternoon\n",
    "            if onset_pre_VNS[i] < time_s < offset_pre_VNS[i]:\n",
    "                date2df.append(date)\n",
    "                session2df.append('pre')\n",
    "                time2df.append(time)\n",
    "                nn_interval2df.append(row['nn_interval'])\n",
    "                subj2df.append(subj_id)\n",
    "            elif offset_pre_VNS[i] < time_s < onset_post_VNS[i]:\n",
    "                date2df.append(date)\n",
    "                session2df.append('during')\n",
    "                time2df.append(time)\n",
    "                nn_interval2df.append(row['nn_interval'])\n",
    "                subj2df.append(subj_id)\n",
    "            elif onset_post_VNS[i] < time_s < offset_post_VNS[i]:\n",
    "                date2df.append(date)\n",
    "                session2df.append('post')\n",
    "                time2df.append(time)\n",
    "                nn_interval2df.append(row['nn_interval'])\n",
    "                subj2df.append(subj_id)\n",
    "            else:\n",
    "                continue\n",
    "df_filtered = pd.DataFrame(data=np.array([np.array(subj2df), np.array(date2df),np.array(time2df), np.array(session2df),\n",
    "                                          np.array(nn_interval2df)]).T,\n",
    "                                    columns=['subj', 'date', 'time', 'session', 'nn_interval'])\n",
    "df_filtered.to_csv(os.path.join(data_dir, 'nn_interval_whole.csv'))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2d8f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset_post_VNS[i]\n",
    "df_info_per_subj_day\n",
    "df_info_per_subj_day[df_info_per_subj_day['switch'] == '0'].reset_index()['time_s']\n",
    "for subj_id in array_subjs[0:1]:\n",
    "    nn_filename = os.path.join(data_dir, str(subj_id) + '.pickle')\n",
    "    df = pd.read_pickle(nn_filename)\n",
    "    # truncate to speed up\n",
    "    df = df.truncate(before=170000, after= 171000)\n",
    "    # reindex by time_s\n",
    "    df['time_s_idx'] = datetime2time_s(df['timestamp'])\n",
    "    df.set_index('time_s_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82afa219-cc46-46a2-9be2-a2e1e5fbe02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258505, 5)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "798658dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2021-02-24T07:13:25.960342000')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['timestamp'].to_numpy()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
